{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dinosaurs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrAlexSanz/Dinosaurs/blob/master/Dinosaurs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yL37UA-bjA_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "6c9df3c7-eff9-4743-f31b-cd51b89fc906"
      },
      "source": [
        "%cd \"/content\"\n",
        "!rm -rf Dinosaurs\n",
        "\n",
        "!git clone https://github.com/DrAlexSanz/Dinosaurs.git\n",
        "  \n",
        "%cd \"/content/Dinosaurs\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'Dinosaurs'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 5 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (5/5), done.\n",
            "/content/Dinosaurs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99hjcsNAbtRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import scipy.io\n",
        "import scipy.misc\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from utils import *\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jLkLpB9jibK",
        "colab_type": "text"
      },
      "source": [
        "Now let's open the dinosaurs file and get a dictionary to translate from character to dataset (the individual characters)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmBX9YoejtRv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2adacaa2-203a-4d2f-dcf1-0e379934a941"
      },
      "source": [
        "data = open(\"dinosaurs.txt\", \"r\").read()\n",
        "data = data.lower()\n",
        "chars = list(set(data))\n",
        "\n",
        "data_size = len(data)\n",
        "vocab_size = len(chars) #Careful, vocabulary is not words, it's a character- level model of language!!!\n",
        "\n",
        "print(\"The total length of the data (Number of characters) is: \" + str(data_size))\n",
        "print(\"The number of total unique characters is: \" + str(vocab_size))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The total length of the data (Number of characters) is: 19909\n",
            "The number of total unique characters is: 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qtdsjStl9Qs",
        "colab_type": "text"
      },
      "source": [
        "So the unique characters are lowercase a to z (26) plus the EOL character (\\n usually). Now I will map the vocabulary to a character. So I will get a dictionary that links: 0 to \\n, 1 to a, 2 to b, etc. I will also get the opposite one, it's cheap, at least in python and with this character set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As4mawjooZEf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9b367cb7-efbd-49b2-b4bb-502cc72796f0"
      },
      "source": [
        "char_to_ix = {ch: i for i, ch in enumerate(sorted(chars))}\n",
        "ix_to_char = {i: ch for i, ch in enumerate(sorted(chars))}\n",
        "\n",
        "print(ix_to_char)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VIIlXwUpSEw",
        "colab_type": "text"
      },
      "source": [
        "Now, this works. Now I have to implement my model. The model has the following parts.\n",
        "\n",
        "\n",
        "\n",
        "*   Initialize parameters.\n",
        "*   Forward pass.\n",
        "*   Backward pass, gradients of loss functions.\n",
        "*   Clip gradients to avoid explosions.\n",
        "*   Update gradients. Go back to forward pass.\n",
        "*   Return parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEXWcyXAqRtv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Clip(gradients, MaxValue):\n",
        "    \"\"\"\n",
        "    This function gets a dictionary with the different gradients and limits the values to +/- MaxValue\n",
        "    \n",
        "    Inputs: gradients, dictionary with the gradients for this step.\n",
        "            MaxValue, A max value to limit the gradients\n",
        "            \n",
        "    Outputs: gradients, a dictionary with the clipped gradients.\n",
        "    \n",
        "    \"\"\"\n",
        "    # Get input gradients\n",
        "    \n",
        "    dWaa = gradients[\"dWaa\"]\n",
        "    dWax = gradients[\"dWax\"]\n",
        "    dWya = gradients[\"dWya\"]\n",
        "    db = gradients[\"db\"]\n",
        "    dby = gradients[\"dby\"]\n",
        "    \n",
        "    # Clip input gradients\n",
        "    \n",
        "    for gradient in [dWaa, dWax, dWya, db, dby]:\n",
        "        \n",
        "        np.clip(gradient, -MaxValue, MaxValue, out = gradient) #gradient is the index of the loop!!!\n",
        "    \n",
        "    gradients = {\"dWaa\": dWaa,\n",
        "                 \"dWax\": dWax,\n",
        "                 \"dWya\": dWya,\n",
        "                 \"db\": db,\n",
        "                 \"dby\": dby}\n",
        "    \n",
        "    return gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGxC4CS2rufp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(13)\n",
        "\n",
        "dWax = np.random.randn(5, 3) * 100\n",
        "dWaa = np.random.randn(5, 5) * 100\n",
        "dWya = np.random.randn(5, 2) * 100\n",
        "db = np.random.randn(5, 1) * 100\n",
        "dby = np.random.randn(5, 1) * 100\n",
        "\n",
        "gradients = {\"dWaa\": dWaa,\n",
        "                 \"dWax\": dWax,\n",
        "                 \"dWya\": dWya,\n",
        "                 \"db\": db,\n",
        "                 \"dby\": dby}\n",
        "\n",
        "MaxValue = 90\n",
        "\n",
        "gradients = Clip(gradients, MaxValue)\n",
        "\n",
        "print(\"dWaa[1][2] = \" + str(gradients[\"dWaa\"][1][2]))\n",
        "print(\"dWax[3][1] = \" + str(gradients[\"dWax\"][3][1]))\n",
        "print(\"dWya[1][2] = \" + str(gradients[\"dWya\"][0][1]))\n",
        "print(\"db[4] = \" + str(gradients[\"db\"][4]))\n",
        "print(\"dby[1] = \" + str(gradients[\"dby\"][1]))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpYa6z2fxyoM",
        "colab_type": "text"
      },
      "source": [
        "Ok, now let's implement the character generation part of the model. The only way to generate some word is to sample characters from the vocabulary. Na√Øvely I could sample randomly, but that's not a great idea. It will look like polish or basque. Here is the workflow:\n",
        "\n",
        "* Pass to the network a dummy input with 0s. $x^{\\langle 1 \\rangle} = \\vec{0}$. Also set $a^{\\langle 0 \\rangle} = \\vec{0}$.\n",
        "* Run one step of forward propagation to get $a^{\\langle 1 \\rangle}$ and $\\hat{y}^{\\langle 1 \\rangle}$. Here are the equations:\n",
        "\n",
        "$$ a^{\\langle t+1 \\rangle} = \\tanh(W_{ax}  x^{\\langle t \\rangle } + W_{aa} a^{\\langle t \\rangle } + b)\\tag{1}$$$$ z^{\\langle t + 1 \\rangle } = W_{ya}  a^{\\langle t + 1 \\rangle } + b_y \\tag{2}$$$$ \\hat{y}^{\\langle t+1 \\rangle } = softmax(z^{\\langle t + 1 \\rangle })\\tag{3}$$\n",
        "\n",
        "Note that $\\hat{y}^{\\langle t+1 \\rangle }$ is a (softmax) probability vector (its entries are between 0 and 1 and sum to 1). $\\hat{y}^{\\langle t+1 \\rangle}_i$ represents the probability that the character indexed by \"i\" is the next character.\n",
        "\n",
        "* Carry out sampling: Pick the next character's index according to the probability distribution specified by $\\hat{y}^{\\langle t+1 \\rangle }$. This means that if $\\hat{y}^{\\langle t+1 \\rangle }_i = 0.16$ I will pick the index \"i\" with 16% probability.\n",
        "\n",
        "* Overwrite the variable x, which currently stores $x^{\\langle t \\rangle }$, with the value of $x^{\\langle t + 1 \\rangle }$. Represent $x^{\\langle t + 1 \\rangle }$ by creating a one-hot vector corresponding to the character chosen as a prediction. Then, forward propagate $x^{\\langle t + 1 \\rangle }$ in Step 1 and keep repeating the process until a \"\\n\" character appears, indicating the end of the dinosaur name.\n"
      ]
    }
  ]
}